# LLM Model Benchmarking Report

Generated: 2025-07-07 20:14:57

## Executive Summary
- **Recommended Model**: gpt-4o-mini
- **Reason**: Highest quality results with good cost-performance balance
- **Best Quality**: gpt-4o-mini (0.859)
- **Most Cost-Effective**: gpt-4o-mini ($0.0051)
- **Fastest**: gpt-4o-mini (120.48s avg)

## Test Configuration
- **Total URLs Tested**: 10
- **Models Compared**: gpt-4o-mini
- **Prompt Template**: default

## Detailed Comparison

| Model | Success Rate | Avg Quality | Total Cost | Cost/URL | Avg Time | Tokens/sec |
|-------|-------------|-------------|------------|----------|----------|------------|
| gpt-4o-mini | 100.0% | 0.859 | $0.0051 | $0.0005 | 120.48s | 123 |

## Quality Analysis

### gpt-4o-mini
- Average Quality Score: 0.859
- Score Distribution:
  - 0.0-0.2: 0 (0.0%)
  - 0.2-0.4: 0 (0.0%)
  - 0.4-0.6: 0 (0.0%)
  - 0.6-0.8: 0 (0.0%)
  - 0.8-1.0: 10 (100.0%)

## Common Issues by Model

## Cost Analysis

| Model | Total Cost | Per URL | Per 1K Tokens |
|-------|-----------|---------|---------------|
| gpt-4o-mini | $0.0051 | $0.0005 | $0.0002 |

## Performance Metrics

### gpt-4o-mini
- Total Duration: 243.16s
- Average per URL: 120.48s
- Tokens per Second: 123
- Total Tokens Used: 29,996

## Recommendations

**Use gpt-4o-mini for production** - Highest quality results with good cost-performance balance

### Model Selection Guide:
- **For highest quality**: Use gpt-4o-mini
- **For lowest cost**: Use gpt-4o-mini
- **For fastest processing**: Use gpt-4o-mini

### Quality Consistency:
- gpt-4o-mini: High consistency (variance: 0.0001)