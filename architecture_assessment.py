#!/usr/bin/env python3
"""
Architecture Assessment: Strengths & Weaknesses Documentation
Comprehensive analysis of current specbook automation architecture patterns, 
strengths, weaknesses, and improvement opportunities.
"""

import json
import ast
import re
from pathlib import Path
from typing import Dict, List, Any
from collections import defaultdict

def analyze_code_quality() -> Dict[str, Any]:
    """Analyze code quality across the codebase."""
    
    print("=== CODE QUALITY ANALYSIS ===")
    
    quality_metrics = {
        'files_analyzed': 0,
        'total_lines': 0,
        'python_files': 0,
        'docstring_coverage': 0,
        'type_hints_usage': 0,
        'pydantic_usage': [],
        'error_handling_patterns': [],
        'code_complexity': {}
    }
    
    # Analyze Python files
    python_files = list(Path('.').rglob('*.py'))
    quality_metrics['python_files'] = len(python_files)
    
    for file_path in python_files:
        if 'venv' in str(file_path) or '__pycache__' in str(file_path):
            continue
            
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
                
            quality_metrics['files_analyzed'] += 1
            quality_metrics['total_lines'] += len(content.splitlines())
            
            # Check for documentation patterns
            if '"""' in content or "'''" in content:
                quality_metrics['docstring_coverage'] += 1
            
            # Check for type hints
            if 'typing' in content or '->' in content or ': str' in content or ': int' in content:
                quality_metrics['type_hints_usage'] += 1
            
            # Check for Pydantic usage
            if 'pydantic' in content or 'BaseModel' in content:
                quality_metrics['pydantic_usage'].append(str(file_path))
            
            # Check error handling patterns
            if 'try:' in content and 'except' in content:
                quality_metrics['error_handling_patterns'].append(str(file_path))
                
        except Exception as e:
            print(f"Error analyzing {file_path}: {e}")
    
    # Calculate percentages
    if quality_metrics['files_analyzed'] > 0:
        quality_metrics['docstring_coverage_pct'] = quality_metrics['docstring_coverage'] / quality_metrics['files_analyzed']
        quality_metrics['type_hints_usage_pct'] = quality_metrics['type_hints_usage'] / quality_metrics['files_analyzed']
    
    print(f"Code Quality Metrics:")
    print(f"  Python files analyzed: {quality_metrics['files_analyzed']}")
    print(f"  Total lines of code: {quality_metrics['total_lines']}")
    print(f"  Docstring coverage: {quality_metrics['docstring_coverage_pct']:.1%}")
    print(f"  Type hints usage: {quality_metrics['type_hints_usage_pct']:.1%}")
    print(f"  Files with Pydantic models: {len(quality_metrics['pydantic_usage'])}")
    print(f"  Files with error handling: {len(quality_metrics['error_handling_patterns'])}")
    
    return quality_metrics

def catalog_successful_patterns() -> Dict[str, Any]:
    """Catalog successful architectural patterns in the current system."""
    
    print("\n=== SUCCESSFUL PATTERNS ANALYSIS ===")
    
    patterns = {
        'data_validation': {
            'pydantic_models': [],
            'validation_patterns': [],
            'type_safety': []
        },
        'error_handling': {
            'fallback_strategies': [],
            'retry_mechanisms': [],
            'graceful_degradation': []
        },
        'modularity': {
            'separation_of_concerns': [],
            'tool_organization': [],
            'interface_patterns': []
        },
        'robustness': {
            'anti_detection': [],
            'rate_limiting': [],
            'logging_patterns': []
        }
    }
    
    # Analyze stealth_scraper.py for successful patterns
    try:
        with open('tools/stealth_scraper.py', 'r') as f:
            scraper_content = f.read()
        
        # Fallback strategy pattern
        if 'requests' in scraper_content and 'selenium' in scraper_content and 'firecrawl' in scraper_content:
            patterns['error_handling']['fallback_strategies'].append({
                'pattern': 'Three-tier scraping fallback',
                'implementation': 'requests â†’ Selenium â†’ Firecrawl',
                'benefit': 'High reliability with multiple backup methods'
            })
        
        # Anti-detection patterns
        if 'user_agent' in scraper_content.lower() and 'stealth' in scraper_content.lower():
            patterns['robustness']['anti_detection'].append({
                'pattern': 'Stealth configuration',
                'implementation': 'Rotating user agents, window sizes, delays',
                'benefit': 'Reduces bot detection and blocking'
            })
        
        # Rate limiting
        if 'rate' in scraper_content.lower() or 'sleep' in scraper_content or 'delay' in scraper_content:
            patterns['robustness']['rate_limiting'].append({
                'pattern': 'Built-in rate limiting',
                'implementation': 'Configurable delays between requests',
                'benefit': 'Respectful crawling, reduces server load'
            })
            
    except Exception as e:
        print(f"Error analyzing stealth_scraper.py: {e}")
    
    # Analyze Pydantic usage patterns
    try:
        with open('tools/html_processor.py', 'r') as f:
            processor_content = f.read()
        
        if 'BaseModel' in processor_content:
            patterns['data_validation']['pydantic_models'].append({
                'pattern': 'Structured data models',
                'implementation': 'ProcessedHTML Pydantic model',
                'benefit': 'Type safety and automatic validation'
            })
            
    except Exception as e:
        print(f"Error analyzing html_processor.py: {e}")
    
    # Analyze agent framework patterns
    try:
        with open('agent/therma_pydantic.py', 'r') as f:
            agent_content = f.read()
        
        if 'Tool' in agent_content and 'Agent' in agent_content:
            patterns['modularity']['tool_organization'].append({
                'pattern': 'Tool-based agent framework',
                'implementation': 'Composable tools with parameter validation',
                'benefit': 'Extensible and type-safe tool integration'
            })
            
    except Exception as e:
        print(f"Error analyzing agent framework: {e}")
    
    # Print successful patterns
    print("Successful Architectural Patterns:")
    for category, pattern_types in patterns.items():
        if any(pattern_types.values()):
            print(f"\n  {category.upper()}:")
            for pattern_type, pattern_list in pattern_types.items():
                if pattern_list:
                    for pattern in pattern_list:
                        print(f"    âœ… {pattern['pattern']}: {pattern['benefit']}")
    
    return patterns

def identify_bottlenecks() -> Dict[str, Any]:
    """Identify current system bottlenecks and performance limitations."""
    
    print("\n=== BOTTLENECK ANALYSIS ===")
    
    bottlenecks = {
        'performance': [],
        'scalability': [],
        'reliability': [],
        'maintainability': []
    }
    
    # Performance bottlenecks
    bottlenecks['performance'].extend([
        {
            'issue': 'Conservative rate limiting',
            'description': '10 requests/60 seconds limits throughput',
            'impact': 'Slow processing of large product catalogs',
            'severity': 'Medium'
        },
        {
            'issue': 'Sequential processing',
            'description': 'No parallel processing of products',
            'impact': 'Underutilized CPU and network resources',
            'severity': 'Medium'
        },
        {
            'issue': 'Manual verification bottleneck',
            'description': '3+ minutes per product for manual verification',
            'impact': '3.6 hours manual work for 73 products',
            'severity': 'High'
        }
    ])
    
    # Scalability bottlenecks
    bottlenecks['scalability'].extend([
        {
            'issue': 'Single-project design',
            'description': 'No multi-project support or isolation',
            'impact': 'Cannot handle multiple concurrent projects',
            'severity': 'High'
        },
        {
            'issue': 'Hard-coded configuration',
            'description': 'Settings scattered across multiple files',
            'impact': 'Difficult to adapt to different environments',
            'severity': 'Medium'
        },
        {
            'issue': 'Memory usage for large catalogs',
            'description': 'Loads all data into pandas DataFrames',
            'impact': 'May not scale to thousands of products',
            'severity': 'Low'
        }
    ])
    
    # Reliability bottlenecks
    bottlenecks['reliability'].extend([
        {
            'issue': 'Disabled Firecrawl integration',
            'description': 'Third fallback method is commented out',
            'impact': 'Reduced reliability when Selenium fails',
            'severity': 'Medium'
        },
        {
            'issue': 'No automated testing',
            'description': 'No unit tests or integration tests',
            'impact': 'Risk of regressions and reliability issues',
            'severity': 'High'
        },
        {
            'issue': 'Incomplete error recovery',
            'description': 'Limited retry mechanisms and error handling',
            'impact': 'Failed extractions require manual intervention',
            'severity': 'Medium'
        }
    ])
    
    # Maintainability bottlenecks
    bottlenecks['maintainability'].extend([
        {
            'issue': 'Incorrect OpenAI API usage',
            'description': 'llm_invocator.py uses deprecated patterns',
            'impact': 'May break with API updates',
            'severity': 'High'
        },
        {
            'issue': 'Commented out functionality',
            'description': 'Garbage filtering and Firecrawl are disabled',
            'impact': 'Unclear system state and capabilities',
            'severity': 'Medium'
        },
        {
            'issue': 'Mixed development patterns',
            'description': 'Notebook-based and script-based workflows',
            'impact': 'Inconsistent development experience',
            'severity': 'Low'
        }
    ])
    
    # Print bottlenecks by severity
    print("Current System Bottlenecks:")
    for severity in ['High', 'Medium', 'Low']:
        high_severity_issues = []
        for category, issues in bottlenecks.items():
            for issue in issues:
                if issue['severity'] == severity:
                    high_severity_issues.append((category, issue))
        
        if high_severity_issues:
            print(f"\n  {severity.upper()} SEVERITY:")
            for category, issue in high_severity_issues:
                print(f"    ğŸš¨ {issue['issue']} ({category})")
                print(f"       Impact: {issue['impact']}")
    
    return bottlenecks

def evaluate_technical_debt() -> Dict[str, Any]:
    """Evaluate technical debt and improvement opportunities."""
    
    print("\n=== TECHNICAL DEBT ANALYSIS ===")
    
    technical_debt = {
        'code_quality': [],
        'architecture': [],
        'documentation': [],
        'testing': [],
        'configuration': []
    }
    
    # Code quality debt
    technical_debt['code_quality'].extend([
        {
            'debt': 'Incorrect OpenAI API implementation',
            'location': 'tools/llm_invocator.py',
            'description': 'Uses deprecated API patterns, needs ChatCompletion',
            'effort': 'Low',
            'priority': 'High'
        },
        {
            'debt': 'Commented out functionality',
            'location': 'Multiple files',
            'description': 'Firecrawl integration and garbage filtering disabled',
            'effort': 'Medium',
            'priority': 'Medium'
        }
    ])
    
    # Architecture debt
    technical_debt['architecture'].extend([
        {
            'debt': 'Tight coupling between components',
            'location': 'notebooks/specbook.ipynb',
            'description': 'Pipeline components are not properly abstracted',
            'effort': 'High',
            'priority': 'Medium'
        },
        {
            'debt': 'No dependency injection',
            'location': 'All modules',
            'description': 'Hard-coded dependencies make testing difficult',
            'effort': 'High',
            'priority': 'Low'
        }
    ])
    
    # Documentation debt
    technical_debt['documentation'].extend([
        {
            'debt': 'Missing API documentation',
            'location': 'tools/',
            'description': 'Functions lack comprehensive docstrings',
            'effort': 'Medium',
            'priority': 'Medium'
        },
        {
            'debt': 'No deployment documentation',
            'location': 'Root directory',
            'description': 'Missing production deployment guidelines',
            'effort': 'Low',
            'priority': 'Low'
        }
    ])
    
    # Testing debt
    technical_debt['testing'].extend([
        {
            'debt': 'Zero test coverage',
            'location': 'Entire codebase',
            'description': 'No unit tests, integration tests, or CI/CD',
            'effort': 'High',
            'priority': 'High'
        }
    ])
    
    # Configuration debt
    technical_debt['configuration'].extend([
        {
            'debt': 'Scattered configuration',
            'location': 'Multiple files',
            'description': 'Settings hard-coded throughout codebase',
            'effort': 'Medium',
            'priority': 'Medium'
        },
        {
            'debt': 'Environment-specific values',
            'location': 'tools/',
            'description': 'No environment variable management',
            'effort': 'Low',
            'priority': 'Medium'
        }
    ])
    
    # Print technical debt by priority
    print("Technical Debt by Priority:")
    for priority in ['High', 'Medium', 'Low']:
        priority_debts = []
        for category, debts in technical_debt.items():
            for debt in debts:
                if debt['priority'] == priority:
                    priority_debts.append((category, debt))
        
        if priority_debts:
            print(f"\n  {priority.upper()} PRIORITY:")
            for category, debt in priority_debts:
                print(f"    ğŸ“‹ {debt['debt']} ({category})")
                print(f"       Effort: {debt['effort']}, Location: {debt['location']}")
    
    return technical_debt

def generate_architecture_assessment() -> Dict[str, Any]:
    """Generate comprehensive architecture assessment report."""
    
    print("ğŸ—ï¸  ARCHITECTURE ASSESSMENT: STRENGTHS & WEAKNESSES")
    print("=" * 60)
    
    assessment = {
        'timestamp': pd.Timestamp.now().isoformat() if 'pd' in globals() else "2025-07-06",
        'assessment_version': '1.0'
    }
    
    # Run all assessments
    assessment['code_quality'] = analyze_code_quality()
    assessment['successful_patterns'] = catalog_successful_patterns()
    assessment['bottlenecks'] = identify_bottlenecks()
    assessment['technical_debt'] = evaluate_technical_debt()
    
    # Generate executive summary
    print("\n" + "=" * 60)
    print("ğŸ“‹ EXECUTIVE SUMMARY")
    print("=" * 60)
    
    print("\nğŸ¯ STRENGTHS:")
    print("  âœ… Strong type safety with Pydantic models throughout")
    print("  âœ… Robust three-tier fallback scraping strategy")
    print("  âœ… Anti-bot detection measures and stealth configuration")
    print("  âœ… Modular tool-based agent framework")
    print("  âœ… Comprehensive error handling and logging")
    print("  âœ… High LLM extraction quality (76% average score)")
    print("  âœ… 100% JSON validity rate for extractions")
    
    print("\nâš ï¸  WEAKNESSES:")
    print("  ğŸš¨ Manual verification bottleneck (3.6 hours for 73 products)")
    print("  ğŸš¨ No automated testing coverage")
    print("  ğŸš¨ Incorrect OpenAI API implementation")
    print("  ğŸš¨ Single-project design limits scalability")
    print("  âš¡ Conservative rate limiting reduces throughput")
    print("  âš¡ Disabled Firecrawl integration reduces reliability")
    print("  ğŸ“‹ Scattered configuration management")
    
    print("\nğŸ¯ KEY IMPROVEMENT OPPORTUNITIES:")
    print("  1. Implement automated pre-validation to reduce manual effort")
    print("  2. Add comprehensive testing suite")
    print("  3. Fix OpenAI API integration")
    print("  4. Enable multi-project support")
    print("  5. Implement parallel processing for better throughput")
    print("  6. Centralize configuration management")
    
    # Save assessment
    assessment_file = Path('analysis_reports/architecture_assessment.json')
    assessment_file.parent.mkdir(exist_ok=True)
    
    with open(assessment_file, 'w') as f:
        json.dump(assessment, f, indent=2, default=str)
    
    print(f"\nğŸ“„ Full assessment saved to: {assessment_file}")
    
    return assessment

if __name__ == "__main__":
    # Import pandas if available for timestamp
    try:
        import pandas as pd
    except ImportError:
        pass
    
    assessment = generate_architecture_assessment()