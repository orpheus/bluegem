{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Setup",
   "id": "5405ce9a3aa5e07f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T04:05:50.596515Z",
     "start_time": "2025-07-01T04:05:50.593436Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import getpass\n",
    "import os\n",
    "from langchain.chat_models import init_chat_model"
   ],
   "id": "f429931adf2185c6",
   "outputs": [],
   "execution_count": 102
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T02:52:51.591548Z",
     "start_time": "2025-07-01T02:52:51.580780Z"
    }
   },
   "cell_type": "code",
   "source": [
    "try:\n",
    "    # load environment variables from .env file (requires `python-dotenv`)\n",
    "    from dotenv import load_dotenv\n",
    "\n",
    "    load_dotenv()\n",
    "except ImportError:\n",
    "    pass"
   ],
   "id": "bca6e370bf457444",
   "outputs": [],
   "execution_count": 83
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-26T05:55:48.426649Z",
     "start_time": "2025-06-26T05:55:48.422009Z"
    }
   },
   "cell_type": "code",
   "source": [
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "if \"LANGSMITH_API_KEY\" not in os.environ:\n",
    "    os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass(\n",
    "        prompt=\"Enter your LangSmith API key (optional): \"\n",
    "    )\n",
    "if \"LANGSMITH_PROJECT\" not in os.environ:\n",
    "    os.environ[\"LANGSMITH_PROJECT\"] = getpass.getpass(\n",
    "        prompt='Enter your LangSmith Project Name (default = \"default\"): '\n",
    "    )\n",
    "    if not os.environ.get(\"LANGSMITH_PROJECT\"):\n",
    "        os.environ[\"LANGSMITH_PROJECT\"] = \"default\""
   ],
   "id": "895458aef02d7811",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-26T05:55:51.691117Z",
     "start_time": "2025-06-26T05:55:49.395725Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")"
   ],
   "id": "18b2055572e46772",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Read product URLs",
   "id": "d02f7df792189e42"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-26T05:55:51.709293Z",
     "start_time": "2025-06-26T05:55:51.704643Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import csv\n",
    "\n",
    "product_urls = []\n",
    "\n",
    "# Open the CSV file\n",
    "with open('specbook.csv', 'r', newline='') as csvfile:\n",
    "    # Create a csv.reader object\n",
    "    reader = csv.reader(csvfile)\n",
    "\n",
    "    for row in reader:\n",
    "        if row[1]:\n",
    "            product_urls.append(row[1])"
   ],
   "id": "ce86d42adcc27e4",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T05:06:23.891165Z",
     "start_time": "2025-07-01T05:06:23.886373Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Check for duplicates\n",
    "print(f\"Total URLs: {len(product_urls)}\")\n",
    "print(f\"Unique URLs: {len(set(product_urls))}\")\n",
    "print(f\"Results: {len(results)}\")\n",
    "\n",
    "# Find duplicates\n",
    "from collections import Counter\n",
    "url_counts = Counter(product_urls)\n",
    "duplicates = {url: count for url, count in url_counts.items() if count > 1}\n",
    "if duplicates:\n",
    "    print(f\"Duplicate URLs: {duplicates}\")"
   ],
   "id": "ec12b53bbbac16fa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total URLs: 87\n",
      "Unique URLs: 82\n",
      "Results: 82\n",
      "Duplicate URLs: {'https://www.dunnedwards.com/colors/browser/dew340': 3, 'https://www.fireclaytile.com/tile/colors/detail/daisy/tile-field-2-x-2': 2, 'https://www.kraususa.com/kraus-khu100-32-32-undermount-16-gauge-stainless-steel-single-bowl-kitchen-sink.html': 2, 'https://www.subzero-wolf.com/wolf/range-hood/46-inch-pro-hood-liner-22-inch-depth': 2}\n"
     ]
    }
   ],
   "execution_count": 139
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Fetch Product HTML",
   "id": "c20c71ee0250d3b3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T03:12:22.404801Z",
     "start_time": "2025-07-01T03:12:22.393644Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import logging\n",
    "\n",
    "# Configure logging for debugging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)"
   ],
   "id": "2e314a728c55905f",
   "outputs": [],
   "execution_count": 85
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T04:53:04.717026Z",
     "start_time": "2025-07-01T04:53:00.663746Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import requests\n",
    "from collections import defaultdict\n",
    "\n",
    "# Just add this function above your existing code:\n",
    "def fetch_url(url):\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        return {\n",
    "            \"code\": response.status_code,\n",
    "            \"text\": response.text,\n",
    "            \"soup\": None\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching {url}: {e}\")  # Add logging\n",
    "        return {\"code\": None, \"text\": None, \"soup\": None, \"error\": str(e)}\n",
    "\n",
    "\n",
    "# Without headers, website responds with 403 b/c it suspects you're a bot\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36\"\n",
    "}\n",
    "\n",
    "# fetch urls\n",
    "results = defaultdict(dict)\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "    for url, result in zip(product_urls, executor.map(fetch_url, product_urls)):\n",
    "        results[url] = result\n",
    "\n",
    "# Add this right after your current code:\n",
    "print(f\"URLs: {len(product_urls)}\")\n",
    "print(f\"Unique URLs: {len(set(product_urls))}\")\n",
    "print(f\"Results: {len(results)}\")"
   ],
   "id": "439db0e45594f123",
   "outputs": [],
   "execution_count": 125
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T04:58:45.465708Z",
     "start_time": "2025-07-01T04:58:40.452994Z"
    }
   },
   "cell_type": "code",
   "source": [
    "soups = {}\n",
    "\n",
    "with open(f'01_llmpipeline/1-requests_failed_{int(time.time())}.csv', 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(['url', 'status_code', 'text'])\n",
    "\n",
    "    for url, response in results.items():\n",
    "        soup = BeautifulSoup(response.get(\"text\"), \"html.parser\")\n",
    "\n",
    "        code = response.get(\"code\")\n",
    "        if code != 200:\n",
    "            writer.writerow([url, code, soup.text])\n",
    "        else:\n",
    "            soups[url] = soup"
   ],
   "id": "59d790b23b98ad2c",
   "outputs": [],
   "execution_count": 132
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Preprocess HTML",
   "id": "daec12cd462a8203"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-02T00:57:00.909430Z",
     "start_time": "2025-07-02T00:57:00.505370Z"
    }
   },
   "cell_type": "code",
   "source": [
    "REMOVE_TAGS = [\n",
    "    \"script\", \"style\", \"noscript\", \"svg\", \"footer\", \"header\",\n",
    "    \"nav\", \"form\", \"iframe\", \"aside\", \"canvas\", \"button\", \"input\", \"select\", \"option\"\n",
    "]\n",
    "\n",
    "GARBAGE_KEYWORDS = [\"cookie\", \"newsletter\", \"subscribe\", \"banner\", \"social\", \"share\", \"advert\"]\n",
    "\n",
    "preprocessed_html = {}\n",
    "\n",
    "for url, soup in soups.items():\n",
    "\n",
    "    # Remove noise tags\n",
    "    for tag in soup(REMOVE_TAGS):\n",
    "        tag.decompose()\n",
    "\n",
    "    # # Remove elements with garbage classes/ids\n",
    "    # for el in soup.find_all(attrs={\"class\": True}):\n",
    "    #     cls = \" \".join(el.get(\"class\"))\n",
    "    #     if any(kw in cls.lower() for kw in GARBAGE_KEYWORDS):\n",
    "    #         el.decompose()\n",
    "    #\n",
    "    # for el in soup.find_all(attrs={\"id\": True}):\n",
    "    #     id_ = el.get(\"id\")\n",
    "    #     if id_ and any(kw in id_.lower() for kw in GARBAGE_KEYWORDS):\n",
    "    #         el.decompose()\n",
    "\n",
    "    # Extract visible text\n",
    "    text = soup.get_text(separator=\"\\n\", strip=True)\n",
    "    text_lines = [line.strip() for line in text.splitlines() if line.strip()]\n",
    "    visible_text = \"\\n\".join(text_lines)\n",
    "\n",
    "    # Extract metadata\n",
    "    metadata = {\n",
    "        tag.get(\"property\") or tag.get(\"name\"): tag.get(\"content\")\n",
    "        for tag in soup.find_all(\"meta\")\n",
    "        if tag.get(\"content\")\n",
    "    }\n",
    "\n",
    "    # Extract images with alt text\n",
    "    images = []\n",
    "    for img in soup.find_all(\"img\"):\n",
    "        src = img.get(\"src\")\n",
    "        alt = img.get(\"alt\", \"\").strip()\n",
    "        if src:\n",
    "            images.append({\"src\": src, \"alt\": alt})\n",
    "\n",
    "    preprocessed_html[url] = {\n",
    "        \"title\": soup.title.string.strip() if soup.title and soup.title.string else \"\",\n",
    "        \"metadata\": metadata,\n",
    "        \"text\": visible_text,\n",
    "        \"images\": images\n",
    "    }\n"
   ],
   "id": "89d160c693f992d9",
   "outputs": [],
   "execution_count": 140
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T04:59:02.904497Z",
     "start_time": "2025-07-01T04:59:02.889196Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "# Max characters after pre-process\n",
    "print(max([len(json.dumps(x)) for x in preprocessed_html]))\n",
    "\n",
    "with open('01_llmpipeline/2-preprocessed_html.json', 'w') as json_file:\n",
    "    json.dump(preprocessed_html, json_file)\n",
    "\n",
    "# with open('preprocessed_html.json', 'r') as json_file:\n",
    "#     preprocessed_html = json.load(json_file)"
   ],
   "id": "c84931487b0b8423",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "203\n"
     ]
    }
   ],
   "execution_count": 135
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Create Prompts",
   "id": "37476b2d2c36474c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-02T01:42:29.065018Z",
     "start_time": "2025-07-02T01:42:28.955579Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "system_template = \"\"\"\n",
    "You are a project architect tasked with fetching specification details from the following product website's HTML page. Extract the relevant product information for documentation in a specification book.\n",
    "\n",
    "If you are not 99.9% sure that the information is correct, return the value with the highest probability, including the probability in the value field.\n",
    "\n",
    "TITLE:\n",
    "data['title']\n",
    "\n",
    "METADATA:\n",
    "data['metadata']\n",
    "\n",
    "TEXT CONTENT:\n",
    "data['text']\n",
    "\n",
    "IMAGES:\n",
    "data['images']\n",
    "\n",
    "Extract the following structured data in JSON format from the provided product web page:\n",
    "\n",
    "- image_url: Direct URL to the product image.\n",
    "- type: The product category (e.g. range hood, grill, fireplace, etc.).\n",
    "- description: Short product description, including brand, size, material, color, and any notable features\n",
    "- model_no: Manufacturer model number, item no, or sku no.\n",
    "- product_link: Original product page URL.\n",
    "- qty: Quantity if specified; otherwise return \"unspecified\".\n",
    "- key: A unique reference key (if available).\n",
    "\n",
    "Return your output in this format. You **don’t add extra formatting instructions yourself**:\n",
    "\n",
    "```json\n",
    "{{\n",
    "  \"image_url\": \"\",\n",
    "  \"type\": \"\",\n",
    "  \"description\": \"\",\n",
    "  \"model_no\": \"\",\n",
    "  \"product_link\": \"\",\n",
    "  \"qty\": \"\",\n",
    "  \"key\": \"\"\n",
    "}}\n",
    "``````\n",
    "data:\n",
    "{data}\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", system_template)]\n",
    ")\n",
    "\n",
    "prompts = {}\n",
    "\n",
    "for url, website_data in preprocessed_html.items():\n",
    "    prompts[url] = prompt_template.invoke({\"data\": json.dumps(website_data)})"
   ],
   "id": "d7e31b34e3d2eab4",
   "outputs": [],
   "execution_count": 152
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-02T01:54:03.570472Z",
     "start_time": "2025-07-02T01:54:03.538962Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open('01_llmpipeline/3-prompts.csv', mode='w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(['url', 'prompt'])\n",
    "\n",
    "    for url, prompt in prompts.items():\n",
    "        writer.writerow([url, prompt.to_string()])\n"
   ],
   "id": "309b685c7af0f252",
   "outputs": [],
   "execution_count": 155
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Invoke Model",
   "id": "b5cbb3b65dd16804"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-02T02:01:23.281155Z",
     "start_time": "2025-07-02T02:01:23.278035Z"
    }
   },
   "cell_type": "code",
   "source": "gpt_4o_mini = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\")",
   "id": "960e896606d29a7f",
   "outputs": [],
   "execution_count": 157
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-02T02:06:58.393786Z",
     "start_time": "2025-07-02T02:02:19.799237Z"
    }
   },
   "cell_type": "code",
   "source": [
    "llm_responses = {}\n",
    "\n",
    "for url, prompt in prompts.items():\n",
    "    llm_responses[url] = gpt_4o_mini.invoke(prompt)"
   ],
   "id": "13e0dbd526326c51",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-01 19:02:22,411 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-07-01 19:02:25,231 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-07-01 19:02:29,802 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-07-01 19:02:32,488 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-07-01 19:02:35,812 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-07-01 19:02:39,937 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-07-01 19:02:42,455 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-07-01 19:02:48,063 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-07-01 19:02:50,830 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-07-01 19:03:00,148 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-07-01 19:03:11,311 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-07-01 19:03:20,095 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-07-01 19:03:26,261 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-07-01 19:03:34,145 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-07-01 19:03:40,697 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-07-01 19:03:48,275 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-07-01 19:03:56,978 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-07-01 19:03:59,495 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-07-01 19:04:05,990 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-07-01 19:04:09,679 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-07-01 19:04:11,316 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-07-01 19:04:12,341 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-07-01 19:04:14,921 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-07-01 19:04:15,973 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-07-01 19:04:16,948 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-07-01 19:04:22,272 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-07-01 19:04:24,087 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-07-01 19:04:28,004 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-07-01 19:04:34,151 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-07-01 19:04:37,220 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-07-01 19:04:40,109 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-07-01 19:04:42,957 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-07-01 19:04:44,493 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-07-01 19:04:50,679 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-07-01 19:04:53,233 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-07-01 19:04:55,829 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-07-01 19:04:56,884 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-07-01 19:04:57,998 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-07-01 19:04:59,437 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-07-01 19:05:03,465 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-07-01 19:05:07,329 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-07-01 19:05:08,456 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-07-01 19:05:11,426 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-07-01 19:05:13,985 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-07-01 19:05:17,262 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-07-01 19:05:20,642 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-07-01 19:05:22,178 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-07-01 19:05:24,738 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-07-01 19:05:27,911 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-07-01 19:05:32,520 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-07-01 19:05:35,181 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-07-01 19:05:38,760 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-07-01 19:05:41,736 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-07-01 19:05:44,735 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-07-01 19:05:45,898 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-07-01 19:05:47,151 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-07-01 19:05:49,825 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-07-01 19:05:53,923 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-07-01 19:05:56,993 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-07-01 19:06:01,079 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-07-01 19:06:04,468 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-07-01 19:06:08,258 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-07-01 19:06:11,433 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-07-01 19:06:14,095 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-07-01 19:06:16,654 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-07-01 19:06:19,247 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-07-01 19:06:23,104 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-07-01 19:06:26,666 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-07-01 19:06:32,732 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-07-01 19:06:37,234 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-07-01 19:06:41,230 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-07-01 19:06:43,728 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-07-01 19:06:46,525 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-07-01 19:06:49,320 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-07-01 19:06:52,597 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-07-01 19:06:56,079 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-07-01 19:06:58,388 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "execution_count": 158
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-02T02:20:34.732212Z",
     "start_time": "2025-07-02T02:20:34.724910Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open('01_llmpipeline/4-llm.csv', mode='w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(['url', 'llm_message'])\n",
    "\n",
    "    for url, llm_message in llm_responses.items():\n",
    "        writer.writerow([url, llm_message.content])"
   ],
   "id": "b68835e08817fdda",
   "outputs": [],
   "execution_count": 165
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "a# Evaluation",
   "id": "f1694a72df8dcd47"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-02T02:26:58.948734Z",
     "start_time": "2025-07-02T02:26:58.912996Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "import re\n",
    "import requests\n",
    "from urllib.parse import urlparse\n",
    "from typing import Dict, List, Tuple, Any\n",
    "from dataclasses import dataclass\n",
    "from collections import defaultdict\n",
    "\n",
    "@dataclass\n",
    "class EvalResult:\n",
    "    \"\"\"Stores evaluation results for a single extraction\"\"\"\n",
    "    url_valid: bool\n",
    "    json_parseable: bool\n",
    "    required_fields_present: bool\n",
    "    field_quality_scores: Dict[str, float]\n",
    "    overall_score: float\n",
    "    issues: List[str]\n",
    "\n",
    "class ProductExtractionEvaluator:\n",
    "    \"\"\"Evaluates LLM product extraction quality\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.required_fields = [\"image_url\", \"type\", \"description\", \"product_link\"]\n",
    "        self.optional_fields = [\"model_no\", \"qty\", \"key\"]\n",
    "\n",
    "    def evaluate_extraction(self, json_str: str, source_url: str = None) -> EvalResult:\n",
    "        \"\"\"\n",
    "        Evaluate a single product extraction\n",
    "\n",
    "        Args:\n",
    "            json_str: The JSON string from LLM\n",
    "            source_url: Original URL that was scraped (optional)\n",
    "\n",
    "        Returns:\n",
    "            EvalResult with detailed scoring\n",
    "        \"\"\"\n",
    "        issues = []\n",
    "        field_scores = {}\n",
    "\n",
    "        # 1. JSON Parseability Test\n",
    "        try:\n",
    "            data = json.loads(json_str)\n",
    "            json_parseable = True\n",
    "        except json.JSONDecodeError as e:\n",
    "            return EvalResult(\n",
    "                url_valid=False,\n",
    "                json_parseable=False,\n",
    "                required_fields_present=False,\n",
    "                field_quality_scores={},\n",
    "                overall_score=0.0,\n",
    "                issues=[f\"JSON parsing failed: {e}\"]\n",
    "            )\n",
    "\n",
    "        # 2. Required Fields Test\n",
    "        missing_fields = [f for f in self.required_fields if f not in data]\n",
    "        required_fields_present = len(missing_fields) == 0\n",
    "        if missing_fields:\n",
    "            issues.append(f\"Missing required fields: {missing_fields}\")\n",
    "\n",
    "        # 3. Field Quality Evaluation\n",
    "        field_scores[\"image_url\"] = self._evaluate_url(data.get(\"image_url\", \"\"))\n",
    "        field_scores[\"product_link\"] = self._evaluate_url(data.get(\"product_link\", \"\"))\n",
    "        field_scores[\"type\"] = self._evaluate_type_field(data.get(\"type\", \"\"))\n",
    "        field_scores[\"description\"] = self._evaluate_description(data.get(\"description\", \"\"))\n",
    "        field_scores[\"model_no\"] = self._evaluate_model_no(data.get(\"model_no\", \"\"))\n",
    "        field_scores[\"qty\"] = self._evaluate_quantity(data.get(\"qty\", \"\"))\n",
    "\n",
    "        # 4. URL Validation\n",
    "        urls_valid = all([\n",
    "            self._is_valid_url(data.get(\"image_url\", \"\")),\n",
    "            self._is_valid_url(data.get(\"product_link\", \"\"))\n",
    "        ])\n",
    "\n",
    "        # 5. Content Consistency Checks\n",
    "        consistency_score = self._check_consistency(data, source_url)\n",
    "        field_scores[\"consistency\"] = consistency_score\n",
    "\n",
    "        # 6. Calculate Overall Score\n",
    "        overall_score = self._calculate_overall_score(field_scores, required_fields_present, urls_valid)\n",
    "\n",
    "        return EvalResult(\n",
    "            url_valid=urls_valid,\n",
    "            json_parseable=json_parseable,\n",
    "            required_fields_present=required_fields_present,\n",
    "            field_quality_scores=field_scores,\n",
    "            overall_score=overall_score,\n",
    "            issues=issues\n",
    "        )\n",
    "\n",
    "    def _evaluate_url(self, url: str) -> float:\n",
    "        \"\"\"Score URL quality (0-1)\"\"\"\n",
    "        if not url or url.strip() == \"\":\n",
    "            return 0.0\n",
    "\n",
    "        if not self._is_valid_url(url):\n",
    "            return 0.2\n",
    "\n",
    "        # Check if it's a reasonable image/product URL\n",
    "        if any(ext in url.lower() for ext in ['.jpg', '.png', '.jpeg', '.webp', '.gif']):\n",
    "            return 1.0\n",
    "        elif 'image' in url.lower() or 'photo' in url.lower() or 'product' in url.lower():\n",
    "            return 0.8\n",
    "        else:\n",
    "            return 0.6\n",
    "\n",
    "    def _is_valid_url(self, url: str) -> bool:\n",
    "        \"\"\"Check if URL is properly formatted\"\"\"\n",
    "        try:\n",
    "            result = urlparse(url)\n",
    "            return all([result.scheme, result.netloc])\n",
    "        except Exception:\n",
    "            return False\n",
    "\n",
    "    def _evaluate_type_field(self, type_val: str) -> float:\n",
    "        \"\"\"Score product type quality\"\"\"\n",
    "        if not type_val or type_val.strip() == \"\":\n",
    "            return 0.0\n",
    "\n",
    "        # Check for reasonable product categories\n",
    "        common_types = [\n",
    "            'furniture', 'electronics', 'clothing', 'kitchen', 'outdoor',\n",
    "            'fireplace', 'appliance', 'tool', 'decoration', 'lighting'\n",
    "        ]\n",
    "\n",
    "        type_lower = type_val.lower()\n",
    "        if any(cat in type_lower for cat in common_types):\n",
    "            return 1.0\n",
    "        elif len(type_val.strip()) > 2:\n",
    "            return 0.7\n",
    "        else:\n",
    "            return 0.3\n",
    "\n",
    "    def _evaluate_description(self, desc: str) -> float:\n",
    "        \"\"\"Score description quality\"\"\"\n",
    "        if not desc or desc.strip() == \"\":\n",
    "            return 0.0\n",
    "\n",
    "        desc_clean = desc.strip()\n",
    "\n",
    "        # Length check\n",
    "        if len(desc_clean) < 10:\n",
    "            return 0.3\n",
    "        elif len(desc_clean) < 50:\n",
    "            return 0.6\n",
    "        elif len(desc_clean) > 500:\n",
    "            return 0.8  # Might be too verbose\n",
    "        else:\n",
    "            return 1.0\n",
    "\n",
    "    def _evaluate_model_no(self, model: str) -> float:\n",
    "        \"\"\"Score model number field\"\"\"\n",
    "        if not model or model.strip() == \"\":\n",
    "            return 0.5  # Neutral - not always available\n",
    "\n",
    "        # Look for typical model patterns\n",
    "        if re.search(r'[A-Z]{2,}[-\\s]?\\d+', model):\n",
    "            return 1.0\n",
    "        elif len(model.strip()) > 2:\n",
    "            return 0.7\n",
    "        else:\n",
    "            return 0.3\n",
    "\n",
    "    def _evaluate_quantity(self, qty: str) -> float:\n",
    "        \"\"\"Score quantity field\"\"\"\n",
    "        if not qty or qty.strip() == \"\":\n",
    "            return 0.5\n",
    "\n",
    "        qty_lower = qty.lower().strip()\n",
    "        if any(word in qty_lower for word in ['unspecified', 'unknown', 'n/a']):\n",
    "            return 0.8  # Honest about not knowing\n",
    "        elif re.search(r'\\d+', qty):\n",
    "            return 1.0\n",
    "        else:\n",
    "            return 0.6\n",
    "\n",
    "    def _check_consistency(self, data: Dict, source_url: str = None) -> float:\n",
    "        \"\"\"Check internal consistency of extracted data\"\"\"\n",
    "        score = 1.0\n",
    "\n",
    "        # Check if product_link and image_url are from same domain\n",
    "        try:\n",
    "            if data.get(\"product_link\") and data.get(\"image_url\"):\n",
    "                prod_domain = urlparse(data[\"product_link\"]).netloc\n",
    "                img_domain = urlparse(data[\"image_url\"]).netloc\n",
    "\n",
    "                if prod_domain and img_domain:\n",
    "                    # Same domain is good\n",
    "                    if prod_domain == img_domain:\n",
    "                        score += 0.1\n",
    "                    # Different but reasonable domains\n",
    "                    elif any(common in prod_domain for common in img_domain.split('.')):\n",
    "                        score += 0.05\n",
    "        except Exception:\n",
    "            score -= 0.1\n",
    "\n",
    "        return min(score, 1.0)\n",
    "\n",
    "    def _calculate_overall_score(self, field_scores: Dict[str, float],\n",
    "                               required_present: bool, urls_valid: bool) -> float:\n",
    "        \"\"\"Calculate weighted overall score\"\"\"\n",
    "        if not required_present:\n",
    "            return 0.2\n",
    "\n",
    "        # Weighted scoring\n",
    "        weights = {\n",
    "            \"image_url\": 0.2,\n",
    "            \"product_link\": 0.2,\n",
    "            \"type\": 0.15,\n",
    "            \"description\": 0.25,\n",
    "            \"model_no\": 0.05,\n",
    "            \"qty\": 0.05,\n",
    "            \"consistency\": 0.1\n",
    "        }\n",
    "\n",
    "        weighted_score = sum(field_scores.get(field, 0) * weight\n",
    "                           for field, weight in weights.items())\n",
    "\n",
    "        # Penalty for invalid URLs\n",
    "        if not urls_valid:\n",
    "            weighted_score *= 0.7\n",
    "\n",
    "        return round(weighted_score, 3)\n",
    "\n",
    "    def evaluate_batch(self, extractions: List[Tuple[str, str]]) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Evaluate multiple extractions and return summary statistics\n",
    "\n",
    "        Args:\n",
    "            extractions: List of (json_string, source_url) tuples\n",
    "\n",
    "        Returns:\n",
    "            Dictionary with batch evaluation results\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        for json_str, source_url in extractions:\n",
    "            result = self.evaluate_extraction(json_str, source_url)\n",
    "            results.append(result)\n",
    "\n",
    "        # Calculate batch statistics\n",
    "        scores = [r.overall_score for r in results]\n",
    "        field_scores = defaultdict(list)\n",
    "\n",
    "        for result in results:\n",
    "            for field, score in result.field_quality_scores.items():\n",
    "                field_scores[field].append(score)\n",
    "\n",
    "        # Aggregate statistics\n",
    "        batch_stats = {\n",
    "            \"total_extractions\": len(results),\n",
    "            \"avg_score\": sum(scores) / len(scores) if scores else 0,\n",
    "            \"min_score\": min(scores) if scores else 0,\n",
    "            \"max_score\": max(scores) if scores else 0,\n",
    "            \"json_parse_success_rate\": sum(1 for r in results if r.json_parseable) / len(results),\n",
    "            \"required_fields_success_rate\": sum(1 for r in results if r.required_fields_present) / len(results),\n",
    "            \"url_validity_rate\": sum(1 for r in results if r.url_valid) / len(results),\n",
    "            \"field_avg_scores\": {\n",
    "                field: sum(scores) / len(scores) if scores else 0\n",
    "                for field, scores in field_scores.items()\n",
    "            },\n",
    "            \"low_quality_extractions\": [\n",
    "                i for i, result in enumerate(results) if result.overall_score < 0.6\n",
    "            ],\n",
    "            \"common_issues\": self._get_common_issues(results)\n",
    "        }\n",
    "\n",
    "        print(\"=== BATCH EVALUATION RESULTS ===\")\n",
    "        print(f\"Total extractions: {batch_stats['total_extractions']}\")\n",
    "        print(f\"Average score: {batch_stats['avg_score']:.3f}\")\n",
    "        print(f\"JSON parse success rate: {batch_stats['json_parse_success_rate']:.2%}\")\n",
    "        print(f\"Required fields success rate: {batch_stats['required_fields_success_rate']:.2%}\")\n",
    "        print(f\"URL validity rate: {batch_stats['url_validity_rate']:.2%}\")\n",
    "        print(\"\\nField Average Scores:\")\n",
    "        for field, score in batch_stats['field_avg_scores'].items():\n",
    "            print(f\"  {field}: {score:.3f}\")\n",
    "\n",
    "        if batch_stats['low_quality_extractions']:\n",
    "            print(f\"\\nLow quality extractions (indices): {batch_stats['low_quality_extractions']}\")\n",
    "\n",
    "        if batch_stats['common_issues']:\n",
    "            print(\"\\nCommon issues:\")\n",
    "            for issue, count in batch_stats['common_issues'].items():\n",
    "                print(f\"  {issue}: {count} occurrences\")\n",
    "\n",
    "        return batch_stats\n",
    "\n",
    "    def _get_common_issues(self, results: List[EvalResult]) -> Dict[str, int]:\n",
    "        \"\"\"Find most common issues across extractions\"\"\"\n",
    "        issue_counts = defaultdict(int)\n",
    "        for result in results:\n",
    "            for issue in result.issues:\n",
    "                issue_counts[issue] += 1\n",
    "        return dict(sorted(issue_counts.items(), key=lambda x: x[1], reverse=True))\n"
   ],
   "id": "e16ef603d4ffe9c7",
   "outputs": [],
   "execution_count": 167
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-02T02:29:36.624856Z",
     "start_time": "2025-07-02T02:29:36.618759Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "\n",
    "evaluations = []\n",
    "\n",
    "def strip_code_blocks(text):\n",
    "    text = re.sub(r'^```\\w*\\n?', '', text, flags=re.MULTILINE)\n",
    "\n",
    "    # Remove closing code block markers\n",
    "    text = re.sub(r'\\n?```$', '', text, flags=re.MULTILINE)\n",
    "\n",
    "    # Clean up any extra whitespace at the beginning and end\n",
    "    return text.strip()\n",
    "\n",
    "for url, llm_response in llm_responses.items():\n",
    "    evaluations.append((strip_code_blocks(llm_response.content), url))"
   ],
   "id": "a4d17ad0357ac669",
   "outputs": [],
   "execution_count": 169
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-02T02:29:46.840966Z",
     "start_time": "2025-07-02T02:29:46.827095Z"
    }
   },
   "cell_type": "code",
   "source": [
    "evaluator = ProductExtractionEvaluator()\n",
    "batch_results = evaluator.evaluate_batch(evaluations)"
   ],
   "id": "496b214ba590aa7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== BATCH EVALUATION RESULTS ===\n",
      "Total extractions: 77\n",
      "Average score: 0.670\n",
      "JSON parse success rate: 100.00%\n",
      "Required fields success rate: 100.00%\n",
      "URL validity rate: 67.53%\n",
      "\n",
      "Field Average Scores:\n",
      "  image_url: 0.623\n",
      "  product_link: 0.561\n",
      "  type: 0.605\n",
      "  description: 0.826\n",
      "  model_no: 0.753\n",
      "  qty: 0.800\n",
      "  consistency: 1.000\n",
      "\n",
      "Low quality extractions (indices): [20, 21, 23, 24, 26, 32, 36, 37, 38, 41, 46, 54, 55, 56, 61, 62, 63, 64, 65, 66, 67, 68, 72, 73, 74]\n"
     ]
    }
   ],
   "execution_count": 171
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Generate Specbook",
   "id": "a33100c55eabc068"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-27T05:32:17.683639Z",
     "start_time": "2025-06-27T05:32:17.677734Z"
    }
   },
   "cell_type": "code",
   "source": [
    "fieldnames = ['image_url', 'type', 'description', 'model_no']\n",
    "\n",
    "with open('01_llmpipeline/5-specbook.csv', 'w', newline='') as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader() # Writes the header row\n",
    "    writer.writerows(filtered_list)"
   ],
   "id": "425d024ea0fb582c",
   "outputs": [],
   "execution_count": 81
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
