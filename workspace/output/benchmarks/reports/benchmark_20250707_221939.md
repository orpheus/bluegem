# LLM Model Benchmarking Report

Generated: 2025-07-07 22:19:39

## Executive Summary
- **Recommended Model**: gpt-3.5-turbo
- **Reason**: Best overall balance of quality, cost, and speed
- **Best Quality**: gpt-4o-mini (0.830)
- **Most Cost-Effective**: gpt-4o-mini ($0.0249)
- **Fastest**: gpt-3.5-turbo (2.22s avg)

## Test Configuration
- **Total URLs Tested**: 50
- **Models Compared**: gpt-4o-mini, gpt-4o, gpt-3.5-turbo
- **Prompt Template**: default

## Detailed Comparison

| Model | Success Rate | Avg Quality | Total Cost | Cost/URL | Avg Time | Tokens/sec |
|-------|-------------|-------------|------------|----------|----------|------------|
| gpt-4o-mini | 100.0% | 0.830 | $0.0249 | $0.0005 | 6.70s | 2011 |
| gpt-4o | 98.0% | 0.823 | $0.7085 | $0.0142 | 30.59s | 401 |
| gpt-3.5-turbo | 94.0% | 0.815 | $0.0582 | $0.0012 | 2.22s | 4151 |

## Quality Analysis

### gpt-4o-mini
- Average Quality Score: 0.830
- Score Distribution:
  - 0.0-0.2: 0 (0.0%)
  - 0.2-0.4: 0 (0.0%)
  - 0.4-0.6: 2 (4.0%)
  - 0.6-0.8: 10 (20.0%)
  - 0.8-1.0: 38 (76.0%)

### gpt-4o
- Average Quality Score: 0.823
- Score Distribution:
  - 0.0-0.2: 0 (0.0%)
  - 0.2-0.4: 0 (0.0%)
  - 0.4-0.6: 3 (6.1%)
  - 0.6-0.8: 10 (20.4%)
  - 0.8-1.0: 36 (73.5%)

### gpt-3.5-turbo
- Average Quality Score: 0.815
- Score Distribution:
  - 0.0-0.2: 0 (0.0%)
  - 0.2-0.4: 0 (0.0%)
  - 0.4-0.6: 3 (6.4%)
  - 0.6-0.8: 8 (17.0%)
  - 0.8-1.0: 36 (76.6%)

## Common Issues by Model

### gpt-4o
- Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-cjP1tSwV63NfDZ1VASS0jo81 on tokens per min (TPM): Limit 30000, Used 12399, Requested 17747. Please try again in 292ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}: 1 occurrences

### gpt-3.5-turbo
- Error code: 400 - {'error': {'message': "This model's maximum context length is 16385 tokens. However, your messages resulted in 21466 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}: 1 occurrences
- Error code: 400 - {'error': {'message': "This model's maximum context length is 16385 tokens. However, your messages resulted in 25507 tokens. Please reduce the length of the messages.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}: 1 occurrences
- Error code: 400 - {'error': {'message': "This model's maximum context length is 16385 tokens. However, you requested 16766 tokens (15766 in the messages, 1000 in the completion). Please reduce the length of the messages or completion.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}: 1 occurrences

## Cost Analysis

| Model | Total Cost | Per URL | Per 1K Tokens |
|-------|-----------|---------|---------------|
| gpt-4o-mini | $0.0249 | $0.0005 | $0.0002 |
| gpt-4o | $0.7085 | $0.0142 | $0.0056 |
| gpt-3.5-turbo | $0.0582 | $0.0012 | $0.0006 |

## Performance Metrics

### gpt-4o-mini
- Total Duration: 71.41s
- Average per URL: 6.70s
- Tokens per Second: 2011
- Total Tokens Used: 143,576

### gpt-4o
- Total Duration: 314.62s
- Average per URL: 30.59s
- Tokens per Second: 401
- Total Tokens Used: 126,210

### gpt-3.5-turbo
- Total Duration: 24.36s
- Average per URL: 2.22s
- Tokens per Second: 4151
- Total Tokens Used: 101,115

## Significant Findings
- Cost varies by more than 2x between models

## Recommendations

**Use gpt-3.5-turbo for production** - Best overall balance of quality, cost, and speed

### Model Selection Guide:
- **For highest quality**: Use gpt-4o-mini
- **For lowest cost**: Use gpt-4o-mini
- **For fastest processing**: Use gpt-3.5-turbo

### Quality Consistency:
- gpt-4o-mini: High consistency (variance: 0.0067)
- gpt-4o: High consistency (variance: 0.0076)
- gpt-3.5-turbo: High consistency (variance: 0.0077)